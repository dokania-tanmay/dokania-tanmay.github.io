---
layout: page
title: Vision-Based Autonomous Maze Navigation
description: An integrated perception and control system built with PyTorch and ROS2 that enables a TurtleBot to solve a physical maze by recognizing and following visual signs.
img: assets/img/7785_cover.gif
importance: 2
category: research
related_publications: 
---
Guide: [Dr. Sean Wilson](https://scholar.google.com/citations?user=Bhz3UroAAAAJ&hl=en), Georgia Institute of Technology.

- Implemented an image preprocessing pipeline using OpenCV and HSV color segmentation to automatically detect, bound, and crop navigational signs from a live camera feed.
- Developed and trained a Convolutional Neural Network (CNN) in PyTorch for multi-class sign recognition, achieving 92% classification accuracy on a withheld test set.
- Architected an autonomous navigation system in ROS2, representing the maze as a discrete grid and managing the robot's pose and orientation using AMCL estimates.
- Created a finite-state machine that integrated the perception output with robot control, translating the classified sign into a target waypoint and orientation for the robot to pursue.
- Fused LIDAR sensor data with localization data to trigger image capture only when the robot was stationary and directly facing a wall, increasing decision-making reliability.
- Successfully deployed the full software stack on a physical TurtleBot, enabling it to autonomously solve the maze by following visual cues with 100% path efficiency.

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/7785_cover.gif" title="video demonstration" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
   Left: Initialization with Pose, Middle: Image classification algorithm, followed by Turtlebot travelling. Right: Robot selecting waypoints based on image.
</div>