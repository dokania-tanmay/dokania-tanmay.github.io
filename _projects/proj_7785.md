---
layout: page
title: Vision-Based Autonomous Maze Navigation
description: An integrated perception and control system built with PyTorch and ROS2 that enables a TurtleBot to solve a physical maze by recognizing and following visual signs.
img: assets/img/7785_cover.gif
importance: 3
category: research
related_publications: 
---
Guide: [Dr. Sean Wilson](https://scholar.google.com/citations?user=Bhz3UroAAAAJ&hl=en), Georgia Institute of Technology.
- Developed a robust sign detection and classification pipeline by preprocessing images with an HSV color-based contouring algorithm in OpenCV  and training a Convolutional Neural Network (CNN) in PyTorch, achieving 93% accuracy on a withheld test set.
- Engineered a real-time, autonomous navigation system in ROS2 by creating a finite-state machine that fused LIDAR data for wall detection with AMCL pose estimates for localization.  This system interpreted the CNN's classifications to generate and pursue waypoints, successfully guiding the robot through a maze with 100% path efficiency. 
- Designed and deployed a full-stack robotics solution on a physical TurtleBot, integrating the PyTorch perception module with a closed-loop control system that made decisions based on multi-modal sensor input (Camera, LIDAR) to navigate from an unknown start to a goal position. 
<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/7785_cover.gif" title="video demonstration" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
   Left: Initialization with Pose, Middle: Image classification algorithm, followed by Turtlebot travelling. Right: Robot selecting waypoints based on image.
</div>